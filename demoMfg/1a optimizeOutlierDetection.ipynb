{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.regularizers import l2\n",
    "import itertools\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "from getConfig  import getConfig\n",
    "from getData    import getData\n",
    "from selectSet import selectSet\n",
    "from getSet import getSet\n",
    "from preProcess import preProcess\n",
    "from calcRMSE import calcRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters for the network\n",
    "def getParms():\n",
    "    L1Size       = [18]\n",
    "    L2Size       = [12]\n",
    "    #L3Size       = [9]\n",
    "    activation   = [\"tanh\"]\n",
    "    batchSize    = [32]\n",
    "    learningRate = [2e-3]\n",
    "    initializer  = [keras.initializers.TruncatedNormal(mean=0.0, stddev=0.5)]\n",
    "    loss         = [\"mse\"]\n",
    "    return list(itertools.product(L1Size,\n",
    "                                  L2Size,\n",
    "                                  #L3Size,\n",
    "                                  activation,\n",
    "                                  batchSize,\n",
    "                                  learningRate,\n",
    "                                  initializer,\n",
    "                                  loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def createVal(d):\n",
    "    # Split Training into Train and Val\n",
    "    valSize = int(d[\"trainX\"].shape[0]*.2)\n",
    "    trainSize = d[\"trainX\"].shape[0] - valSize\n",
    "    d[\"trainX\"] = d[\"trainX\"].head(trainSize)\n",
    "    d[\"trainY\"] = d[\"trainY\"].head(trainSize)\n",
    "    d[\"valX\"] = d[\"trainX\"].tail(valSize)\n",
    "    d[\"valY\"] = d[\"trainY\"].tail(valSize)\n",
    "    \n",
    "    del d['testX'][\"unit\"]   # Not needed for outlier detection\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def buildLayers(parmDict, featureCount):\n",
    "    Dense   = partial(keras.layers.Dense,\\\n",
    "                      kernel_initializer=parmDict[\"initializer\"])\n",
    "    Dropout = partial(keras.layers.Dropout)\n",
    "    \n",
    "    nn = tf.keras.Sequential()\n",
    "    nn.add(Dense(parmDict[\"l1Size\"], activation=parmDict[\"activation\"]))\n",
    "    nn.add(Dropout(0.25))\n",
    "    nn.add(Dense(parmDict[\"l2Size\"], activation=parmDict[\"activation\"]))\n",
    "    nn.add(Dropout(0.25))\n",
    "    #nn.add(Dense(parmDict[\"l3Size\"], activation=parmDict[\"activation\"]))\n",
    "    #nn.add(Dense(parmDict[\"l2Size\"], activation=parmDict[\"activation\"]))\n",
    "    #nn.add(layers.Dense(parmDict[\"l2Size\"], activation=parmDict[\"activation\"]))\n",
    "    #nn.add(layers.Dense(parmDict[\"l1Size\"], activation=parmDict[\"activation\"]))\n",
    "    nn.add(Dense(featureCount, activation=parmDict[\"activation\"]))\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def buildNetwork(parmDict, featureCount):\n",
    "    nn = buildLayers(parmDict, featureCount)\n",
    "    sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    #model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "    nn.compile(optimizer=sgd, loss=\"mse\")\n",
    "    '''nn.compile(optimizer=tf.train.AdamOptimizer(parmDict[\"lr\"]),\\\n",
    "               loss=parmDict[\"loss\"],\\\n",
    "               metrics=['accuracy'])'''\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fitNetwork(dataDict, parmDict, nn):\n",
    "    TB        = keras.callbacks.TensorBoard(log_dir=config[\"TBdir\"])\n",
    "    #modelSave = keras.callbacks.ModelCheckpoint(config[\"modelDir\"],\n",
    "    #                                            save_weights_only=True,\n",
    "    #                                            verbose=1)\n",
    "    \n",
    "    data = np.array(dataDict[\"trainX\"])\n",
    "    nn.fit(data, data,\\\n",
    "                  batch_size=parmDict[\"batchSize\"],\\\n",
    "                  epochs=40,\\\n",
    "                  validation_split=0.15,\\\n",
    "                  verbose=0,\\\n",
    "                  shuffle=True,\n",
    "                  callbacks=[TB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scoreNetwork(df, nn):\n",
    "    data  = np.array(df)\n",
    "    preds = nn.predict(x=data)\n",
    "    rmse  = calcRMSE(df, preds)\n",
    "    # rmse is a series: one value for each column. Return the avg across all columns\n",
    "    return rmse.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a number from 1-41\n",
      "\n",
      "Processing set  1\n",
      "removing setting3\n",
      "removing sensor1\n",
      "removing sensor5\n",
      "removing sensor10\n",
      "removing sensor16\n",
      "removing sensor18\n",
      "removing sensor19\n"
     ]
    }
   ],
   "source": [
    "config = getConfig()\n",
    "parms = getParms()\n",
    "\n",
    "Set = selectSet()\n",
    "\n",
    "train, test = getData(config)\n",
    "train = getSet(train, Set)\n",
    "test = getSet(test, Set)\n",
    "\n",
    "dataDict = preProcess(train, test, config)\n",
    "dataDict = createVal(dataDict)\n",
    "\n",
    "featureCount = dataDict[\"trainX\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for x in parms:\n",
    "    parmDict = {}                  # holds the hyperparameter combination for one run\n",
    "    parmDict['l1Size']      = x[0]\n",
    "    parmDict['l2Size']      = x[1]\n",
    "    #parmDict['l3Size']      = x[2]\n",
    "    parmDict['activation']  = x[2]\n",
    "    parmDict['batchSize']   = x[3]\n",
    "    parmDict['lr']          = x[4]\n",
    "    parmDict['initializer'] = x[5]\n",
    "    parmDict['loss']        = x[6]\n",
    "    \n",
    "    nn = buildNetwork(parmDict, featureCount)\n",
    "    fitNetwork(dataDict, parmDict, nn)\n",
    "    score = scoreNetwork(dataDict[\"testX\"], nn)\n",
    "    tup = (x, score)\n",
    "    results.append(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parms                              RMSE    \n",
      "(18, 12, 'tanh', 32, 0.002, <tensorflow.python.keras.initializers.TruncatedNormal object at 0x7f79e77b2be0>, 'mse')0.38\n"
     ]
    }
   ],
   "source": [
    "print(\"{:<35}{:<8}\".format(\"Parms\", \"RMSE\") )\n",
    "for r in results:\n",
    "    print(\"{:<35}{:.2f}\".format(str(r[0]), r[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the model when you're optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.save_model(nn, filepath=\"autoencoder.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13096, 18)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
